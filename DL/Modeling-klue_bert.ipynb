{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 11:50:07.656080: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 11:50:47.813701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-03 11:50:47.813921: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-03 11:50:47.813930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import mlflow\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# file_name = \"./dataset/cleaned_data.csv\"\n",
    "# file_name = \"./dataset/preprocessed_data.csv\"\n",
    "file_name = \"./dataset/merged_data.csv\"\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "df = df.rename({'Sentence' : 'text', 'Emotion' : 'label'}, axis=1)\n",
    "LABEL = 'label'\n",
    "\n",
    "# df > X, y\n",
    "X, y = df.drop(LABEL, axis=1), df.loc[:, LABEL]\n",
    "\n",
    "# encoding label\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "df['label'] = y = label_encoder.transform(y) # range(7) > ['anger', 'disgust', 'fear', 'happiness', 'neutralism', 'sadness', 'surprise']\n",
    "# df['label'] = y = label_encoder.transform(y) # range(7) > ['당황', '분노', '불안', '슬픔', '중립', '행복', '혐오']\n",
    "\n",
    "\n",
    "n_classes = len(label_encoder.classes_)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "trainset, testset = train_test_split(df, test_size=0.3, random_state=42, shuffle=True, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['당황', '분노', '불안', '슬픔', '중립', '행복', '혐오'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.inverse_transform(range(7))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    66000.000000\n",
       "mean        70.652303\n",
       "std         43.518038\n",
       "min          1.000000\n",
       "25%         26.000000\n",
       "50%         75.000000\n",
       "75%        107.000000\n",
       "max        307.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset['text'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28287.000000\n",
       "mean        70.904514\n",
       "std         43.758114\n",
       "min          1.000000\n",
       "25%         26.000000\n",
       "50%         76.000000\n",
       "75%        107.000000\n",
       "max        300.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset['text'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/klue/bert-base\n",
    "\n",
    "# model = AutoModel.from_pretrained(CHECKPOINT)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "CHECKPOINT = \"klue/bert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer_pretrained=CHECKPOINT, token_max_length=512):\n",
    "        self.data = dataframe        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_pretrained)\n",
    "        self.token_max_len = token_max_length\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # 토큰화 처리\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,                \n",
    "            return_tensors='pt',     \n",
    "            truncation=True,         \n",
    "            padding='max_length',    \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.token_max_len # max_len\n",
    "        )\n",
    "\n",
    "        input_ids = tokens['input_ids'].squeeze(0)           # 2D -> 1D\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0) # 2D -> 1D\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "        }, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = dict(\n",
    "    token_len = 64,\n",
    "    batch_size = 16,\n",
    "    drop_out = 0.5,\n",
    ")\n",
    "\n",
    "train_data = TokenDataset(trainset, CHECKPOINT, model_params['token_len'])\n",
    "test_data = TokenDataset(testset, CHECKPOINT, model_params['token_len'])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=model_params['batch_size'], shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_data, batch_size=model_params['batch_size'], shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(torch.nn.Module):\n",
    "    def __init__(self, bert_pretrained=CHECKPOINT, dropout_rate=0.3):\n",
    "        super(Bert, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_pretrained)\n",
    "        self.dr = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.fc = torch.nn.Linear(768, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bert(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dr): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = Bert(bert_pretrained=CHECKPOINT, dropout_rate=model_params['drop_out'])\n",
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = dict(\n",
    "    lr = 2e-5,\n",
    "    weight_decay = 1e-2,\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(bert.parameters(), lr=train_params['lr'], weight_decay=train_params['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def model_train(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "    counts = 0\n",
    "    \n",
    "    prograss_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)\n",
    "    \n",
    "    for idx, (inputs, labels) in enumerate(prograss_bar):\n",
    "        inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(**inputs)\n",
    "        loss = loss_fn(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, pred = output.max(dim=1)\n",
    "        \n",
    "        corr += pred.eq(labels).sum().item()\n",
    "        counts += len(labels)\n",
    "        \n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        \n",
    "        prograss_bar.set_description(f\"training loss: {running_loss/(idx+1):.5f}, training accuracy: {corr / counts:.5f}\")\n",
    "        \n",
    "    acc = corr / len(data_loader.dataset)\n",
    "    \n",
    "    return running_loss / len(data_loader.dataset), acc\n",
    "\n",
    "def model_evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "        \n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(**inputs)\n",
    "            \n",
    "            _, pred = output.max(dim=1)\n",
    "            \n",
    "            corr += torch.sum(pred.eq(labels)).item()\n",
    "            \n",
    "            running_loss += loss_fn(output, labels).item() * labels.size(0)\n",
    "        \n",
    "        acc = corr / len(data_loader.dataset)\n",
    "        \n",
    "        return running_loss / len(data_loader.dataset), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# mlflow.set_tracking_uri('mlruns')\n",
    "# mlflow.set_experiment(\"Emotion_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 15.66510, training accuracy: 0.62673: 100%|██████████| 4125/4125 [16:58<00:00,  4.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from inf to 0.85483. Saving Model!\n",
      "epoch 01, loss: 0.97907, acc: 0.62673, val_loss: 0.85483, val_accuracy: 0.67625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 12.14566, training accuracy: 0.71364: 100%|██████████| 4125/4125 [16:45<00:00,  4.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.85483 to 0.84886. Saving Model!\n",
      "epoch 02, loss: 0.75910, acc: 0.71364, val_loss: 0.84886, val_accuracy: 0.67982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 9.49531, training accuracy: 0.77935: 100%|██████████| 4125/4125 [16:11<00:00,  4.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 03, loss: 0.59346, acc: 0.77935, val_loss: 0.92891, val_accuracy: 0.68282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 6.72797, training accuracy: 0.84794: 100%|██████████| 4125/4125 [16:49<00:00,  4.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 04, loss: 0.42050, acc: 0.84794, val_loss: 1.08062, val_accuracy: 0.67699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 4.47698, training accuracy: 0.90112: 100%|██████████| 4125/4125 [16:38<00:00,  4.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 05, loss: 0.27981, acc: 0.90112, val_loss: 1.29202, val_accuracy: 0.67176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 3.07474, training accuracy: 0.93347: 100%|██████████| 4125/4125 [16:48<00:00,  4.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 06, loss: 0.19217, acc: 0.93347, val_loss: 1.41084, val_accuracy: 0.67155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 2.20168, training accuracy: 0.95238: 100%|██████████| 4125/4125 [16:21<00:00,  4.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 07, loss: 0.13760, acc: 0.95238, val_loss: 1.54625, val_accuracy: 0.66904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 1.72831, training accuracy: 0.96227: 100%|██████████| 4125/4125 [16:25<00:00,  4.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 08, loss: 0.10802, acc: 0.96227, val_loss: 1.65987, val_accuracy: 0.67010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 1.44992, training accuracy: 0.96906: 100%|██████████| 4125/4125 [16:17<00:00,  4.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 09, loss: 0.09062, acc: 0.96906, val_loss: 1.65875, val_accuracy: 0.67377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 1.26367, training accuracy: 0.97285: 100%|██████████| 4125/4125 [16:17<00:00,  4.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 0.07898, acc: 0.97285, val_loss: 1.73524, val_accuracy: 0.67165\n",
      "best val loss at 1\n"
     ]
    }
   ],
   "source": [
    "# 에폭별 모델 저장하도록 코드 수정\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "\n",
    "model_base_path = \"models/multi-dataset\"\n",
    "if not os.path.exists(model_base_path):\n",
    "    os.mkdir(model_base_path)\n",
    "    \n",
    "model_name_base = 'klue-bert-' + \"-\".join([k+\"_\"+str(v) for k, v in model_params.items()] +[k+\"_\"+str(v) for k, v in train_params.items()])\n",
    "\n",
    "num_epochs = 10\n",
    "min_loss = np.inf\n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = model_train(bert, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss, val_acc = model_evaluate(bert, test_loader, loss_fn, device)\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        print(f'[INFO] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "        min_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    torch.save(bert.state_dict(), f'{model_base_path + model_name_base} + {epoch}.pth')\n",
    "    \n",
    "    print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, acc: {train_acc:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_acc:.5f}')\n",
    "\n",
    "os.rename(f'{model_base_path + model_name_base} + {best_epoch}.pth', f'{model_base_path + model_name_base} + {best_epoch}_best.pth')\n",
    "print(f'best val loss at {best_epoch}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier():\n",
    "    def __init__(self, model, tokenizer, labels: dict):\n",
    "        model.to(device)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        \n",
    "    def predict(self, sentence):\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,                # 1개 문장 \n",
    "            return_tensors='pt',     # 텐서로 반환\n",
    "            truncation=True,         # 잘라내기 적용\n",
    "            padding='max_length',    # 패딩 적용\n",
    "            add_special_tokens=True, # 스페셜 토큰 적용\n",
    "            max_length = 64,\n",
    "        )\n",
    "        tokens.to(device)\n",
    "        prediction = self.model(**tokens)\n",
    "        prediction = torch.nn.functional.softmax(prediction, dim=1)\n",
    "        output = prediction.argmax(dim=1).item()\n",
    "        prob, result = prediction.max(dim=1)[0].item(), self.labels[output]\n",
    "        print(f'[{result}]\\n확률은: {prob*100:.3f}% 입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Bert()\n",
    "model_path = '/home/wonhong/workspace/Emotion_Detection/DL/models/multi-datasetklue-bert-token_len_64-batch_size_16-drop_out_0.5-lr_2e-05-weight_decay_0.01 + 9.pth'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# ['anger', 'disgust', 'fear', 'happiness', 'neutralism', 'sadness', 'surprise']\n",
    "# ['당황', '분노', '불안', '슬픔', '중립', '행복', '혐오']\n",
    "label_dict = {idx:label for idx, label in enumerate(label_encoder.inverse_transform(range(n_classes)))}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "ecf = EmotionClassifier(model, tokenizer, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[분노]\n",
      "확률은: 99.866% 입니다.\n"
     ]
    }
   ],
   "source": [
    "ecf.predict(df.sample(1)['text'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE :  어찌 노인네들이 자기주장만 펼치는지 아무튼 그런 노인들이 너무 싫고 그런 상황이 슬퍼.노인정에서 어디를 놀러 가기로 했는데 서로 자기주장만 펼치잖아. 이 상황에 환멸이 나.서로 조금씩 양보하자고 건의해보려고. 그러면 조금씩 이해하는 마음이 생기지 않을까 해.\n",
      "[슬픔]\n",
      "확률은: 99.994% 입니다.\n",
      "CPU times: user 68.3 ms, sys: 0 ns, total: 68.3 ms\n",
      "Wall time: 68.4 ms\n",
      "3\n",
      "------------------------------\n",
      "SENTENCE :  일을 안 다니니 창피해서 동창 모임에 안 나갈 거야.다들 잘나가고 결혼도 하고 직업도 좋은데 나만 백수야.응. 스스로 자신감이 없어.\n",
      "[당황]\n",
      "확률은: 99.978% 입니다.\n",
      "CPU times: user 37.6 ms, sys: 0 ns, total: 37.6 ms\n",
      "Wall time: 37.7 ms\n",
      "0\n",
      "------------------------------\n",
      "SENTENCE :  길을 걸어가다가 넘어졌는데 아내가 너무 웃어서 부끄러웠어.주위에 사람이 엄청 많아서 그런지 시선들이 느껴져서 더 부끄러웠어.집에 가서 아내에게 한 마디 해야겠어.\n",
      "[당황]\n",
      "확률은: 99.988% 입니다.\n",
      "CPU times: user 29 ms, sys: 9.41 ms, total: 38.4 ms\n",
      "Wall time: 38.3 ms\n",
      "0\n",
      "------------------------------\n",
      "SENTENCE :  드디어 회사 프로젝트가 끝나서 너무 기뻐.당분간은 야근이 없을 것 같아.못 만난 친구들과 수다도 떨고 맛있는 걸 먹으러 돌아다니고 싶어.\n",
      "[행복]\n",
      "확률은: 99.993% 입니다.\n",
      "CPU times: user 28.2 ms, sys: 0 ns, total: 28.2 ms\n",
      "Wall time: 28.3 ms\n",
      "5\n",
      "------------------------------\n",
      "SENTENCE :  나 요즘 시험점수때문에 많이 힘들어.최근 내가 공부한것만큼 성과가 안 나오는것 같아. 공부도 예전만큼 썩 안되는 것 같고.맞아. 아무래도 약간 휴식을 가지고 마음을 정리할 시간이 필요한 것 같아.\n",
      "[슬픔]\n",
      "확률은: 99.597% 입니다.\n",
      "CPU times: user 34.7 ms, sys: 0 ns, total: 34.7 ms\n",
      "Wall time: 34.7 ms\n",
      "3\n",
      "------------------------------\n",
      "SENTENCE :  삶이 힘드네요어떻게 해쳐나가야 할지 막막합니다!!\n",
      "[슬픔]\n",
      "확률은: 99.927% 입니다.\n",
      "CPU times: user 29.6 ms, sys: 0 ns, total: 29.6 ms\n",
      "Wall time: 29.6 ms\n",
      "3\n",
      "------------------------------\n",
      "SENTENCE :  늘 하던 작업인데 오늘따라 일이 잘 안 풀리네. 왜 이렇게 오래 걸리는지 모르겠어.요즘 들어 실수도 많아진 것 같고 이제 나보다 잘하는 사람도 많은 것 같아 불안해.나태해지지 말고 두 배 세배 더 노력해야겠지.\n",
      "[당황]\n",
      "확률은: 99.768% 입니다.\n",
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 15.9 ms\n",
      "0\n",
      "------------------------------\n",
      "SENTENCE :  연애로 기분이 좋아지는 것 같아.배려해주고 아껴줘서 사랑받는 느낌이 들어.덕분에 마음의 안정을 찾았어. 나도 받은 만큼 많이 사랑해줄래.\n",
      "[행복]\n",
      "확률은: 99.974% 입니다.\n",
      "CPU times: user 15.3 ms, sys: 0 ns, total: 15.3 ms\n",
      "Wall time: 15.3 ms\n",
      "5\n",
      "------------------------------\n",
      "SENTENCE :  병원에서 치매 진단을 받았는데 아들이 집에서 같이 살자고 해서 행복해.믿을 수 있는 아들이 있으니 행복한데 부담될까 봐 걱정되기도 해.약도 꼬박꼬박 먹고 열심히 운동해서 아들에게 피해가 되지 말아야지.\n",
      "[행복]\n",
      "확률은: 99.902% 입니다.\n",
      "CPU times: user 14.8 ms, sys: 0 ns, total: 14.8 ms\n",
      "Wall time: 14.8 ms\n",
      "5\n",
      "------------------------------\n",
      "SENTENCE :  애잔하네요\n",
      "[슬픔]\n",
      "확률은: 99.078% 입니다.\n",
      "CPU times: user 14.5 ms, sys: 0 ns, total: 14.5 ms\n",
      "Wall time: 14.6 ms\n",
      "3\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for predict_times in range(10):\n",
    "    tmp_df = df.sample(1)\n",
    "    sentence = tmp_df['text'].values[0]\n",
    "    ans = tmp_df['label'].values[0]\n",
    "    print(\"SENTENCE : \", sentence)\n",
    "    %time ecf.predict(sentence)\n",
    "    print(ans)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
