{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 14:38:10.915199: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 14:38:21.574541: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-23 14:38:21.574773: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-23 14:38:21.574788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 14:38:42.117819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-23 14:38:42.391526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-23 14:38:42.391854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-23 14:38:42.410888: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 14:38:42.414493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-23 14:38:42.414870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-23 14:38:42.415136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-23 14:38:47.096832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-23 14:38:47.115008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-23 14:38:47.115042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-23 14:38:47.115450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-23 14:38:47.118111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3920 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizerFast\n",
    "\n",
    "# https://huggingface.co/klue/bert-base\n",
    "\n",
    "CHECKPOINT = \"klue/bert-base\" \n",
    "bert_base = TFBertForSequenceClassification.from_pretrained(CHECKPOINT, from_pt=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "LABEL = 'label'\n",
    "\n",
    "# df > X, y\n",
    "X, y = df.drop(LABEL, axis=1), df.loc[:, LABEL]\n",
    "\n",
    "# encoding label\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y = label_encoder.transform(y) # range(7) > ['anger', 'disgust', 'fear', 'happiness', 'neutralism', 'sadness', 'surprise']\n",
    "n_classes = len(label_encoder.classes_)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 14:45:58.286557: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 20378736 exceeds 10% of free system memory.\n",
      "2023-04-23 14:45:58.345071: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 20378736 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_encoded = tokenizer(list(X_train['text'].values), truncation=True, padding=True, max_length=512, return_tensors='tf')\n",
    "text_encoded = tokenizer(list(X_test['text'].values), truncation=True, padding=True, max_length=512, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "input_ids = Input(batch_shape=(None, 512), dtype=tf.int32, name='input_ids')\n",
    "input_masks = Input(batch_shape=(None, 512), dtype=tf.int32, name='attention_masks')\n",
    "embedding = bert_base([input_ids, input_masks])[0]\n",
    "y_output = keras.layers.Dense(n_classes, activation='softmax')(embedding)\n",
    "\n",
    "model = Model(inputs=[input_ids, input_masks], outputs=y_output, name='Bert_classification_1')\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 189), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 189), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 189), dtype=tf.int32, name=None)},),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_encoded)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/polymorphism/function_cache.py:117\u001b[0m, in \u001b[0;36mFunctionCache.lookup\u001b[0;34m(self, key, use_function_subtyping)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_function_subtyping:\n\u001b[1;32m    115\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_primary\u001b[39m.\u001b[39mget(key, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 117\u001b[0m dispatch_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch_table\u001b[39m.\u001b[39;49mdispatch(key)\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m dispatch_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_primary[dispatch_key]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/polymorphism/type_dispatch.py:78\u001b[0m, in \u001b[0;36mTypeDispatchTable.dispatch\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the deepest subtype target if it exists in the table.\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m# For known exact matches.\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[39mif\u001b[39;00m request \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch_table:\n\u001b[1;32m     79\u001b[0m   \u001b[39mreturn\u001b[39;00m request\n\u001b[1;32m     81\u001b[0m \u001b[39m# For known non-exact matches.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# (self._dispatch cache does not contain exact matches)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/polymorphism/function_cache.py:77\u001b[0m, in \u001b[0;36mFunctionCacheKey.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39;49m((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_context, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/polymorphism/function_type.py:246\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39;49m(\n\u001b[1;32m    247\u001b[0m       (\u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters\u001b[39m.\u001b[39;49mitems()), \u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaptures\u001b[39m.\u001b[39;49mitems())))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/polymorphism/function_type.py:106\u001b[0m, in \u001b[0;36mParameter.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 106\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39;49m((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkind, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtype_constraint))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/trace_type/default_types.py:207\u001b[0m, in \u001b[0;36mTuple.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 207\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcomponents)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/trace_type/default_types.py:207\u001b[0m, in \u001b[0;36mTuple.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 207\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcomponents)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/trace_type/default_types.py:584\u001b[0m, in \u001b[0;36mReference.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 584\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39;49m((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49midentifier, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase))\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 189), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 189), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 189), dtype=tf.int32, name=None)},),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>"
     ]
    }
   ],
   "source": [
    "model.fit(train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 14:39:15.739239: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 20378736 exceeds 10% of free system memory.\n",
      "2023-04-23 14:39:24.578557: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 20378736 exceeds 10% of free system memory.\n",
      "2023-04-23 14:39:33.387732: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 20378736 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encoded), y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(text_encoded), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train_model(model, train_params, train_dataset, test_dataset):\n",
    "    if train_params['optimizer'] == 'Adam':\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=train_params['learning_rate'],\n",
    "            weight_decay=train_params['weight_decay']\n",
    "        )\n",
    "    elif train_params['optimizer'] == 'AdamW':\n",
    "        optimizer = tfa.optimizers.AdamW(\n",
    "            learning_rate=train_params['learning_rate'],\n",
    "            weight_decay=train_params['weight_decay']\n",
    "        )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=model.compute_loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=train_params['patience'],\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    check = ModelCheckpoint(\n",
    "        filepath='./klue_bert_tuned.h5',\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "    hist = model.fit(train_dataset,\n",
    "                     validation_data=test_dataset,\n",
    "                     epochs=train_params['epochs'],\n",
    "                     callbacks=[early, check])\n",
    "    return hist, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = \"klue/bert-base\" \n",
    "bert = TFBertForSequenceClassification.from_pretrained(CHECKPOINT,\n",
    "                                                        from_pt=True,\n",
    "                                                        num_labels=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/wonhong/workspace/Emotion_Detection/DL/dataset/mlruns/812122781326073109', creation_time=1682257502675, experiment_id='812122781326073109', last_update_time=1682257502675, lifecycle_stage='active', name='Emotion_classfication', tags={}>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('./mlruns')\n",
    "mlflow.set_experiment(\"Emotion_classfication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = dict(\n",
    "    optimizer = 'Adam', # Adam, AdamW\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay = None, # None, 1e-5 etc\n",
    "    patience = 5,\n",
    "    epochs = 30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/23 13:55:03 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: The layer \"tf_bert_for_sequence_classification_1\" has never been called and thus has no defined input shape. Note that the `input_shape` property is only available for Functional and Sequential models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/transformers/src/transformers/modeling_tf_utils.py\", line 1567, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filewiom9sms.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_file0gzcef59.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_filewiom9sms.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_filem_xm0qio.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_bert_for_sequence_classification_1' (type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/transformers/src/transformers/modeling_tf_utils.py\", line 1639, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/transformers/src/transformers/models/bert/modeling_tf_bert.py\", line 1651, in call  *\n            outputs = self.bert(\n        File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_filewiom9sms.py\", line 36, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/tmp/__autograph_generated_filem_xm0qio.py\", line 75, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"/transformers/src/transformers/modeling_tf_utils.py\", line 1639, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/transformers/src/transformers/models/bert/modeling_tf_bert.py\", line 774, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'bert' (type TFBertMainLayer):\n          • self=tf.Tensor(shape=(189,), dtype=int32)\n          • input_ids=None\n          • attention_mask=tf.Tensor(shape=(189,), dtype=int32)\n          • token_type_ids=tf.Tensor(shape=(189,), dtype=int32)\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_bert_for_sequence_classification_1' (type TFBertForSequenceClassification):\n      • self={'input_ids': 'tf.Tensor(shape=(189,), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(189,), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(189,), dtype=int32)'}\n      • input_ids=None\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbaseline\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39mrun_name):\n\u001b[0;32m----> 8\u001b[0m     train_model(model,\n\u001b[1;32m      9\u001b[0m                 train_params,\n\u001b[1;32m     10\u001b[0m                 train_dataset,\n\u001b[1;32m     11\u001b[0m                 test_dataset)\n\u001b[1;32m     13\u001b[0m     y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_dataset)\n\u001b[1;32m     14\u001b[0m     acc \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[43], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_params, train_dataset, test_dataset)\u001b[0m\n\u001b[1;32m     21\u001b[0m early \u001b[39m=\u001b[39m EarlyStopping(\n\u001b[1;32m     22\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m     min_delta\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m check \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     30\u001b[0m     filepath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./klue_bert_tuned.h5\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     33\u001b[0m     save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataset,\n\u001b[1;32m     37\u001b[0m                  validation_data\u001b[39m=\u001b[39;49mtest_dataset,\n\u001b[1;32m     38\u001b[0m                  epochs\u001b[39m=\u001b[39;49mtrain_params[\u001b[39m'\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     39\u001b[0m                  callbacks\u001b[39m=\u001b[39;49m[early, check])\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m hist, model\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/utils/autologging_utils/safety.py:552\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m try_log_autologging_event(\n\u001b[1;32m    543\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_start,\n\u001b[1;32m    544\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m     kwargs,\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m \u001b[39mif\u001b[39;00m patch_is_class:\n\u001b[0;32m--> 552\u001b[0m     patch_function\u001b[39m.\u001b[39;49mcall(call_original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    553\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/utils/autologging_utils/safety.py:170\u001b[0m, in \u001b[0;36mPatchFunction.call\u001b[0;34m(cls, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mcls\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/utils/autologging_utils/safety.py:181\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_exception(e)\n\u001b[1;32m    178\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[39m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/utils/autologging_utils/safety.py:174\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    173\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_patch_implementation(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/utils/autologging_utils/safety.py:232\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mlflow\u001b[39m.\u001b[39mactive_run():\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[0;32m--> 232\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_patch_implementation(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run:\n\u001b[1;32m    235\u001b[0m     mlflow\u001b[39m.\u001b[39mend_run(RunStatus\u001b[39m.\u001b[39mto_string(RunStatus\u001b[39m.\u001b[39mFINISHED))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/tensorflow/__init__.py:1255\u001b[0m, in \u001b[0;36mautolog.<locals>.FitPatch._patch_implementation\u001b[0;34m(self, original, inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1252\u001b[0m early_stop_callback \u001b[39m=\u001b[39m _get_early_stop_callback(callbacks)\n\u001b[1;32m   1253\u001b[0m _log_early_stop_callback_params(early_stop_callback)\n\u001b[0;32m-> 1255\u001b[0m history \u001b[39m=\u001b[39m original(inst, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1257\u001b[0m \u001b[39mif\u001b[39;00m log_models:\n\u001b[1;32m   1258\u001b[0m     _log_keras_model(history, args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/utils/autologging_utils/safety.py:535\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[1;32m    533\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/utils/autologging_utils/safety.py:470\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    463\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    464\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m         og_kwargs,\n\u001b[1;32m    469\u001b[0m     )\n\u001b[0;32m--> 470\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39;49mog_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mog_kwargs)\n\u001b[1;32m    472\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    473\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    474\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m         og_kwargs,\n\u001b[1;32m    479\u001b[0m     )\n\u001b[1;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/utils/autologging_utils/safety.py:532\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    529\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    530\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    531\u001b[0m ):\n\u001b[0;32m--> 532\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39;49m_og_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_og_kwargs)\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filene_r3tp3.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/transformers/src/transformers/modeling_tf_utils.py:1567\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1566\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1567\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_using_dummy_loss:\n\u001b[1;32m   1569\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_loss(y_pred\u001b[39m.\u001b[39mloss, y_pred\u001b[39m.\u001b[39mloss, sample_weight, regularization_losses\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filewiom9sms.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file0gzcef59.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m      9\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mbert, (), \u001b[39mdict\u001b[39m(input_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(input_ids), attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(attention_mask), token_type_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(token_type_ids), position_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(position_ids), head_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(head_mask), inputs_embeds\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(inputs_embeds), output_attentions\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_hidden_states), return_dict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(return_dict), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     12\u001b[0m pooled_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(outputs)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m pooled_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdropout, (), \u001b[39mdict\u001b[39m(inputs\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(pooled_output), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filewiom9sms.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filem_xm0qio.py:75\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     73\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_((\u001b[39mlambda\u001b[39;00m : (ag__\u001b[39m.\u001b[39mld(input_ids) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)), (\u001b[39mlambda\u001b[39;00m : (ag__\u001b[39m.\u001b[39mld(inputs_embeds) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m))), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m (batch_size, seq_length) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_shape)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/transformers/src/transformers/modeling_tf_utils.py\", line 1567, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filewiom9sms.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_file0gzcef59.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_filewiom9sms.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_filem_xm0qio.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_bert_for_sequence_classification_1' (type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/transformers/src/transformers/modeling_tf_utils.py\", line 1639, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/transformers/src/transformers/models/bert/modeling_tf_bert.py\", line 1651, in call  *\n            outputs = self.bert(\n        File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_filewiom9sms.py\", line 36, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/tmp/__autograph_generated_filem_xm0qio.py\", line 75, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"/transformers/src/transformers/modeling_tf_utils.py\", line 1639, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/transformers/src/transformers/models/bert/modeling_tf_bert.py\", line 774, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'bert' (type TFBertMainLayer):\n          • self=tf.Tensor(shape=(189,), dtype=int32)\n          • input_ids=None\n          • attention_mask=tf.Tensor(shape=(189,), dtype=int32)\n          • token_type_ids=tf.Tensor(shape=(189,), dtype=int32)\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_bert_for_sequence_classification_1' (type TFBertForSequenceClassification):\n      • self={'input_ids': 'tf.Tensor(shape=(189,), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(189,), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(189,), dtype=int32)'}\n      • input_ids=None\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "run_name = \"baseline\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    train_model(model,\n",
    "                train_params,\n",
    "                train_dataset,\n",
    "                test_dataset)\n",
    "    \n",
    "    y_pred = model.predict(test_dataset)\n",
    "    acc = accuracy_score(y_test, y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
